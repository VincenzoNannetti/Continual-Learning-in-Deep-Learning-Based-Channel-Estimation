# --- UNet Baseline Configuration ---
# Description: Configuration for training a standalone UNet model.

experiment_name: unet_baseline_run # Change this for each specific run

framework:
  seed: 42             # Seed for reproducibility

model:
  name: unet        
  params:
    in_channels: 2      
    base_features: 48    
  pretrained_path: C:\Users\Vincenzo_DES\OneDrive - Imperial College London\Year 4\ELEC70017 - Individual Project\Main\Learning_Algorithms\results\checkpoints\unet\baseline_unet_trained_on_datasetA\best_model.pth
  strict_load: True      

data:
  dataset_type: channel    
  data_source: self        
  data_dir_self: ./Learning_Algorithms/data/raw/My Data/ 
  data_dir_paper: ./Learning_Algorithms/data/raw/Paper Data/ 
  preprocessed_dir: ./Learning_Algorithms/data/preprocessed/ 
  snr: 20                  
  data_name: all_test_data_SNR20_no_agg_scatter_4blocks 
  dataset_a_name: all_test_data_SNR20_no_agg_scatter_4blocks
  dataset_b_name: all_test_data_SNR0_with_agg_scatter_4blocks_datasetB
  num_pilots: 48           # Number of pilots (used for interpolation)
  interpolation: rbf       # Interpolation method ('rbf', 'spline', etc.)
  normalisation: zscore    # Normalisation method ('zscore', 'minmax', 'none')
  normalise_before_interp: True 
  normalise_target: True   # Whether to normalise the target data using input stats
  num_workers: 0           # DataLoader workers (0 often best for non-huge datasets)
  validation_split: 0.15   # Fraction for validation
  test_split: 0.15         # Fraction for testing

training:
  epochs: 100              # Number of training epochs
  batch_size: 64           # Adjust based on GPU memory
  loss_function: huber       # Common loss functions: 'mse', 'mae', 'huber'
  # huber_delta: 1.0       # Delta for Huber loss (if used)
  optimiser: adam          # 'adam' is a common default
  learning_rate: 2.523297961373369e-05      # Starting learning rate
  weight_decay: 0          # L2 regularization (e.g., 1e-5)
  # --- Adam specific params ---
  betas: [0.9, 0.999]
  eps: 1e-8
  # --- Scheduler ---
  scheduler: True          # Enable LR scheduling
  scheduler_type: ReduceLROnPlateau # Good default
  scheduler_factor: 0.1    # Factor to reduce LR by
  scheduler_patience: 10   # Epochs to wait for improvement
  scheduler_min_lr: 1e-6   # Minimum learning rate
  # --- Early Stopping ---
  early_stopping_patience: 15 # Epochs without val loss improvement before stopping

evaluation:
  metrics: [nmse, psnr, mse, ssim]
  # save_results_dir: ./Learning_Algorithms/results/evaluations/unet/ # Directory for evaluation results
  save_results_dir: ./Learning_Algorithms/Catastrophic_Forgetting/results/unet/finetuned_eval_a
  
logging:
  checkpoint_dir: ./Learning_Algorithms/results/checkpoints/unet/ 

hardware:
  device: auto           
  use_amp: True            
