# --- Base Configuration Template ---
# Description: Template for running channel estimation experiments.
#              Copy and modify this file for specific experiments.

experiment_name: baseline_unet_snr22 # Change this for each experiment

framework:
  seed: 42             # Seed for reproducibility (PyTorch, NumPy, random splits)

model:
  name: unet           # Model name (e.g., unet, srcnn, dncnn, combined, unet_combined, srcnn_supermask, unet_supermask)
  params: {}           # Model-specific initalisation parameters (e.g., {in_channels: 2, out_channels: 2, depth: 4})
  pretrained_path: null # Optional: Path to a .pth file with pretrained weights for the whole model
  strict_load: True    # Whether to strictly enforce matching keys when loading pretrained_path
  # --- For Combined Models (e.g., unet_combined, combined) ---
  # Specify paths to pretrained components if loading them separately
  pretrained_unet: null  # Example: ./Learning_Algorithms/results/checkpoints/unet_baseline/unet_baseline_best.pth
  pretrained_srcnn: null # Example: ./Learning_Algorithms/results/checkpoints/srcnn_baseline/srcnn_baseline_best.pth
  # Specify which parts to freeze during training (if applicable, depends on model implementation)
  freeze_unet: False
  freeze_srcnn: False

data:
  dataset_type: channel    # 'channel' (for baseline models) or 'supermask'
  data_dir: ./Learning_Algorithms/data/raw/My_Data/ # Path to raw .mat files
  preprocessed_dir: ./Learning_Algorithms/data/preprocessed/default/ # Base directory for preprocessed .npy files (subdirs might be created based on params)
  data_source: self        # 'self' or 'paper'
  snr: 22                  # SNR level of the data
  data_name: SNR22_smallRX # Specific name/identifier for the dataset (used by dataset loader)
  num_pilots: 48           # Number of pilots (used for interpolation if applicable)
  interpolation: rbf       # Interpolation method ('rbf', 'spline', 'linear', 'nearest', 'cubic')
  normalisation: zscore    # normalisation method ('zscore', 'minmax', 'log', 'none')
  normalise_before_interp: True # Apply normalisation before or after interpolation
  num_workers: 0           # Number of workers for DataLoader (0 for main process)
  validation_split: 0.15   # Fraction of data for validation set
  test_split: 0.15         # Fraction of data for test set

training:
  epochs: 100
  batch_size: 128
  loss_function: mse       # 'mse' or 'mae' ('l1')
  optimiser: adam          # 'adam' or 'sgd'
  learning_rate: 1e-4
  weight_decay: 1e-5       # L2 regularization
  # --- Adam specific params (used if optimiser is adam) ---
  betas: [0.9, 0.999]
  eps: 1e-8
  # --- SGD specific params (used if optimiser is sgd) ---
  # momentum: 0.9
  # --- Scheduler ---
  scheduler: True          # Enable learning rate scheduler
  scheduler_type: ReduceLROnPlateau # 'ReduceLROnPlateau', 'StepLR', 'CosineAnnealingLR', etc.
  # Params for ReduceLROnPlateau
  scheduler_factor: 0.5
  scheduler_patience: 5
  scheduler_min_lr: 1e-6
  # Params for StepLR
  # scheduler_step_size: 30
  # scheduler_gamma: 0.1
  # --- Early Stopping ---
  early_stopping_patience: 15 # Number of epochs to wait for improvement before stopping

evaluation:
  metrics: [nmse, psnr]    # List of metrics to calculate (lowercase names from src/utils/metrics.py)
  # Optional: Path to save detailed evaluation results (YAML) - relative to workspace root
  save_results_path: null  # Example: ./Learning_Algorithms/results/evaluations/experiment_name_eval.yaml

logging:
  # Directory to save model checkpoints (.pth files) - relative to workspace root
  checkpoint_dir: ./Learning_Algorithms/results/checkpoints/default/ # Experiment-specific subdirs often created based on experiment_name

hardware:
  device: auto             # 'auto', 'cuda', 'cpu'
  use_amp: True            # Enable Automatic Mixed Precision (requires device='cuda')

# --- Supermask Specific Configuration (only used if model name contains 'supermask') ---
supermask:
  tasks: 10                # Number of tasks/masks
  # Path to the frozen pre-trained baseline model (used if model.pretrained_path is null for the supermask model itself)
  pretrained_baseline_path: null # Example: ./Learning_Algorithms/results/checkpoints/unet_baseline/unet_baseline_best.pth
  # Input channels for supermask model (2 or 3) - relevant for data loading
  input_channels: 3 
  # --- Evaluation specific for supermasks ---
  # evaluation.task_id: 0 # Specify which task to evaluate (if mode is evaluate/test) 