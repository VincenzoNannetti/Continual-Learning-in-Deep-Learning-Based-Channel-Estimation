# --- Combined (UNet+SRCNN) Model Configuration ---

experiment_name: combined_unet_srcnn_mixed_dataset

framework:
  seed: 42

model:
  name: combined_unet_srcnn   
  params: {  base_features: 48 }                # Keep empty unless sub-models need specific args here

  pretrained_path: null #C:\Users\Vincenzo_DES\OneDrive - Imperial College London\Year 4\ELEC70017 - Individual Project\Main\Learning_Algorithms\results\checkpoints\unet\unet_srcnn_finetuned_on_datasetB\best_model.pth
  strict_load: True # Using default from load_model
  
  # UNet specific parameters
  pretrained_unet: null      # <--- Set path to trained UNet .pth if available
  freeze_unet: False         # <--- Set True to freeze UNet weights

  # SRCNN specific parameters
  pretrained_srcnn: null       # <--- Set path to trained SRCNN .pth if available
  freeze_srcnn: False          # <--- Set True to freeze SRCNN weights

data:
  # --- Keep your data settings the same ---
  dataset_type: channel
  data_dir_self: ./Learning_Algorithms/data/raw/My Data/ # Ensure this key is correct
  # data_dir_paper: ... # Add if needed
  preprocessed_dir: ./Learning_Algorithms/data/preprocessed/
  data_source: self 
  snr: 0 
  data_name: all_test_data_mixed_dataset_4blocks_dynamic_pilots
  dataset_a_name: all_test_data_SNR20_no_agg_scatter_4blocks
  dataset_b_name: all_test_data_SNR0_with_agg_scatter_4blocks_datasetB
  num_pilots: 48
  interpolation: rbf 
  normalisation: zscore 
  normalise_before_interp: True 
  normalise_target: True   
  num_workers: 0
  validation_split: 0.15
  test_split: 0.15

training:
  # --- Adjust training settings as needed ---
  epochs: 100 
  batch_size: 256 
  loss_function: mse 
  optimiser: adam 
  learning_rate: 0.0007630194900076013 
  weight_decay: 1e-5 
  betas: [0.9, 0.999]
  eps: 1e-8
  scheduler: True 
  scheduler_type: ReduceLROnPlateau
  scheduler_factor: 0.1 
  scheduler_patience: 5 
  scheduler_min_lr: 1e-6
  early_stopping_patience: 15 

evaluation:
  metrics: [nmse, psnr, mse, ssim]
  # save_results_dir: ./Learning_Algorithms/Catastrophic_Forgetting/results/unet_srcnn/finetuned_eval_a

logging:
  checkpoint_dir: ./Learning_Algorithms/results/checkpoints/unet/

hardware:
  device: auto
  use_amp: True 