# --- Combined (UNet+SRCNN) Model Configuration ---

experiment_name: combined_unet_srcnn_mixed_dataset

framework:
  seed: 42

model:
  name: combined_unet_srcnn   
  params: {  base_features: 48 }                # Keep empty unless sub-models need specific args here

  pretrained_path: null #C:\Users\Vincenzo_DES\OneDrive - Imperial College London\Year 4\ELEC70017 - Individual Project\Main\Learning_Algorithms\results\checkpoints\unet\unet_srcnn_finetuned_on_datasetB\best_model.pth
  strict_load: True # Using default from load_model
  
  # UNet specific parameters
  pretrained_unet: null        # <--- Set path to trained UNet .pth if available
  freeze_unet: False           # <--- Set True to freeze UNet weights

  # SRCNN specific parameters
  pretrained_srcnn: null       # <--- Set path to trained SRCNN .pth if available
  freeze_srcnn: False          # <--- Set True to freeze SRCNN weights

data:
  # --- Keep your data settings the same ---
  dataset_type: standard
  data_dir: ./data/raw/original/ # Ensure this key is correct
  preprocessed_dir: ./data/preprocessed/original/

  data_name: original_datasetA
  dataset_a_name: original_datasetA
  dataset_b_name: original_datasetB
 
  interpolation: rbf 
  normalisation: zscore 
  normalise_before_interp: True 
  normalise_target: True   
  
  num_workers: 0
  validation_split: 0.15
  test_split: 0.15

training:
  # --- Adjust training settings as needed ---
  epochs: 100 
  batch_size: 256 
  loss_function: mse 
  optimiser: adam 
  learning_rate: 0.0007630194900076013 
  weight_decay: 1e-5 
  betas: [0.9, 0.999]
  eps: 1e-8
  scheduler: True 
  scheduler_type: ReduceLROnPlateau
  scheduler_factor: 0.1 
  scheduler_patience: 5 
  scheduler_min_lr: 1e-6
  early_stopping_patience: 15 

evaluation:
  metrics: [nmse, psnr, mse, ssim]
  save_results_dir: ./standard_training/checkpoints/unet/evaluations/

logging:
  checkpoint_dir: ./standard_training/checkpoints/unet/

hardware:
  device: auto
  use_amp: True 