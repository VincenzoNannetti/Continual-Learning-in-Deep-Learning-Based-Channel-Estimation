# --- Combined (Autoencoder+SRCNN) Model Configuration ---

experiment_name: combined_res_ae_srcnn # Changed name

framework:
  seed: 42

model:
  name: combined_ae_srcnn   
  params: {}                # Keep empty unless sub-models need specific args here
  strict_load: True
  
  # Autoencoder specific parameters
  autoencoder_type: residual   # 'residual' Or 'basic' - Choose based on CombinedModel_AESRCNN.py
  pretrained_autoencoder: C:\Users\Vincenzo_DES\OneDrive - Imperial College London\Year 4\ELEC70017 - Individual Project\Main\Learning_Algorithms\results\checkpoints\autoencoder\autoencoder_srcnn_finetuned_on_datasetB\best_model.pth

  # SRCNN specific parameters
  pretrained_srcnn: null       # <--- Set path to trained SRCNN .pth if available
  freeze_srcnn: False          # <--- Set True to freeze SRCNN weights

data:
  # --- Keep your data settings the same as combined_model_training.yaml ---
  dataset_type: channel
  data_dir_self: ./Learning_Algorithms/data/raw/My Data/ # Ensure this key is correct
  # data_dir_paper: ... # Add if needed
  preprocessed_dir: ./Learning_Algorithms/data/preprocessed/
  data_source: self 
  snr: 20 
  data_name: all_test_data_SNR20_no_agg_scatter_4blocks
  dataset_a_name: all_test_data_SNR20_no_agg_scatter_4blocks
  dataset_b_name: all_test_data_SNR0_with_agg_scatter_4blocks_datasetB
  num_pilots: 48
  interpolation: rbf 
  normalisation: minmax 
  normalise_before_interp: True 
  normalise_target: True
  num_workers: 0
  validation_split: 0.15
  test_split: 0.15

training:
  # --- Adjust training settings as needed, or use values from previous config ---
  epochs: 100 
  batch_size: 64
  loss_function: huber  # / mse / mae / huber
  # huber_delta: 0.5
  optimiser: adam 
  learning_rate: 9.374308686706442e-05
  weight_decay: 0.0001 
  betas: [0.9, 0.999]
  eps: 1e-8
  scheduler: True 
  scheduler_type: ReduceLROnPlateau
  scheduler_factor: 0.1 
  scheduler_patience: 5 
  scheduler_min_lr: 1e-6
  early_stopping_patience: 15 

evaluation:
  metrics: [nmse, psnr, mse, ssim]
  save_results_dir: ./Learning_Algorithms/Catastrophic_Forgetting/results/ae_srcnn/finetuned_eval_a

logging:
  checkpoint_dir: ./Learning_Algorithms/results/checkpoints/autoencoder/ # Optional: unique checkpoint dir

hardware:
  device: auto
  use_amp: True 