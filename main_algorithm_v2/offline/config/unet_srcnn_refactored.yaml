experiment_name: unet_srcnn_lora_refactored

framework:
  seed: 42

model:
  name: UNet_SRCNN_LoRA
  pretrained_path: C:\Users\Vincenzo_DES\OneDrive - Imperial College London\Year 4\ELEC70017 - Individual Project\Project\standard_training_2\optuna_study_results\trial_1_unet_srcnn\checkpoints\best_model.pth
  evaluation_path: null #C:\Users\Vincenzo_DES\OneDrive - Imperial College London\Year 4\ELEC70017 - Individual Project\Project\main_algorithm\offline\checkpoints\lora\unet_srcnn_lora_refactored-20250606_193203\lora_run_checkpoint.pth
  params:
    # Option for domain-specific batch normalisation
    use_domain_specific_bn: True

    # LoRA-specific parameters
    lora_r: 8
    lora_alpha: 16
    lora_dropout: 0.0
    lora_bias_trainable: 'none'
    
    # Task-specific LoRA parameters
    task_lora_ranks:
      0: 8
      1: 3
      2: 5
      3: 13
      4: 9
      5: 4
      6: 4
      7: 54
      8: 23
    task_lora_alphas:
      0: 1
      1: 1
      2: 1
      3: 1
      4: 1
      5: 1
      6: 1
      7: 1
      8: 1
    task_lora_dropouts:
      0: 0.0
      1: 0.0
      2: 0.0
      3: 0.0
      4: 0.0
      5: 0.0
      6: 0.0
      7: 0.0
      8: 0.0
    
    # Architecture parameters (CORRECTED to match backbone model)
    # These will be overridden by the backbone checkpoint anyway, but should match for clarity
    unet_args:
      in_channels: 2
      base_features: 32
      use_batch_norm: false
      depth: 3
      activation: leakyrelu
      use_leaky_relu: false
      leaky_slope: 0.19873055480755272
      verbose: false
    
    # SRCNN parameters (CORRECTED to match backbone model)
    srcnn_channels: [128, 64]
    srcnn_kernels: [7, 3, 5]
    num_tasks_for_model: 9

  strict_load: True

data:
  dataset_type: standard
  data_dir: ./data/raw/ray_tracing/
  preprocessed_dir: ./data/preprocessed/
  tasks: 9
  sequence: [0, 1, 2, 3, 4, 5, 6, 7, 8]
  tasks_params:
    0:
      data_name: domain_high_snr_med_linear_cl.mat
      snr: 20
    1:
      data_name: domain_high_snr_slow_linear_cl.mat
      snr: 20
    2:
      data_name: domain_high_snr_fast_linear_cl.mat
      snr: 20
    3:
      data_name: domain_low_snr_slow_linear_cl.mat
      snr: 3
    4:
      data_name: domain_low_snr_med_linear_cl.mat
      snr: 3
    5:
      data_name: domain_low_snr_fast_linear_cl.mat
      snr: 3
    6:
      data_name: domain_med_snr_slow_linear_cl.mat
      snr: 10
    7:
      data_name: domain_med_snr_med_linear_cl.mat
      snr: 10
    8:
      data_name: domain_med_snr_fast_linear_cl.mat
      snr: 10
  interpolation: thin_plate_spline
  normalisation: zscore
  normalise_target: True
  # Normalisation stats will be loaded from the backbone checkpoint
  # norm_stats: 
  #   mean_inputs: ...
  #   std_inputs: ...
  #   mean_targets: ...
  #   std_targets: ...
  num_workers: 0
  validation_split: 0.15
  test_split: 0.15

training:
  epochs_per_task: 6
  batch_size: 32
  loss_function: mse
  optimiser: adam
  learning_rate: 0.005
  weight_decay: 0.0001
  task_weight_decays:
    0: 0.0001
    1: 0.0001
    2: 0.0001
    3: 0.0001
    4: 0.0001
    5: 0.0001
    6: 0.0001
    7: 0.0001
    8: 0.0001
  betas: [0.9, 0.999]
  early_stopping_patience: 5
  scheduler:
    type: ReduceLROnPlateau
    params:
      mode: min
      factor: 0.5
      patience: 2
      min_lr: 0.000001
      verbose: True

evaluation:
  metrics: [nmse, psnr, ssim, mse]
  plot_n_examples: 3

logging:
  checkpoint_dir: ./main_algorithm_v2/offline/checkpoints/lora/

hardware:
  device: auto
  use_amp: True 