experiment_name: unet_srcnn_lora_refactored

framework:
  seed: 42

model:
  name: UNet_SRCNN_LoRA
  pretrained_path: C:\Users\Vincenzo_DES\OneDrive - Imperial College London\Year 4\ELEC70017 - Individual Project\Project\standard_training_2\checkpoints\unet_srcnn\best_model.pth #C:\Users\Vincenzo_DES\OneDrive - Imperial College London\Year 4\ELEC70017 - Individual Project\Project\standard_training_2\optuna_study_results\trial_1_unet_srcnn\checkpoints\best_model.pth
  evaluation_path: null #C:\Users\Vincenzo_DES\OneDrive - Imperial College London\Year 4\ELEC70017 - Individual Project\Project\main_algorithm\offline\checkpoints\lora\unet_srcnn_lora_refactored-20250606_193203\lora_run_checkpoint.pth
  params:
    # Option for domain-specific batch normalisation
    use_domain_specific_bn: True

    # LoRA-specific parameters
    lora_r: 8
    lora_alpha: 16
    lora_dropout: 0.0
    lora_bias_trainable: 'none'
    
    # Task-specific LoRA parameters (optimized for domain difficulty)
    task_lora_ranks:
      0: 4   # High SNR (20 dB) - smaller ranks for easier domains
      1: 4   # High SNR (20 dB)
      2: 4   # High SNR (20 dB)
      3: 16  # Low SNR (3 dB) - larger ranks for harder domains
      4: 16  # Low SNR (3 dB)
      5: 16  # Low SNR (3 dB)
      6: 4  # Medium SNR (10 dB) - medium ranks
      7: 4  # Medium SNR (10 dB)
      8: 4   # Medium SNR (10 dB)
    task_lora_alphas:
      0: 4  # Match rank for high SNR
      1: 4  # Match rank for high SNR
      2: 4  # Match rank for high SNR
      3: 6  # Match rank for low SNR
      4: 6  # Match rank for low SNR
      5: 6  # Match rank for low SNR
      6: 4  # Match rank for medium SNR
      7: 4  # Match rank for medium SNR
      8: 4  # Match rank for medium SNR
    task_lora_dropouts:
      0: 0.0
      1: 0.0
      2: 0.0
      3: 0.0
      4: 0.0
      5: 0.0
      6: 0.0
      7: 0.0
      8: 0.0
    
    # Architecture parameters (CORRECTED to match backbone model)
    # These will be overridden by the backbone checkpoint anyway, but should match for clarity
    unet_args:
      in_channels: 2
      base_features: 16
      use_batch_norm: false
      depth: 2
      activation: leakyrelu
      use_leaky_relu: false
      leaky_slope: 0.01
      verbose: false
    
    # SRCNN parameters (CORRECTED to match backbone model)
    srcnn_channels: [64, 32]
    srcnn_kernels: [9, 1, 5]
    num_tasks_for_model: 9

  strict_load: True

data:
  dataset_type: standard
  data_dir: ../../data/raw/ray_tracing/
  preprocessed_dir: ../../data/preprocessed/
  tasks: 9
  sequence: [0, 1, 2, 3, 4, 5, 6, 7, 8]
  tasks_params:
    0:
      data_name: domain_high_snr_med_linear_cl.mat
      snr: 20
    1:
      data_name: domain_high_snr_slow_linear_cl.mat
      snr: 20
    2:
      data_name: domain_high_snr_fast_linear_cl.mat
      snr: 20
    3:
      data_name: domain_low_snr_slow_linear_cl.mat
      snr: 3
    4:
      data_name: domain_low_snr_med_linear_cl.mat
      snr: 3
    5:
      data_name: domain_low_snr_fast_linear_cl.mat
      snr: 3
    6:
      data_name: domain_med_snr_slow_linear_cl.mat
      snr: 10
    7:
      data_name: domain_med_snr_med_linear_cl.mat
      snr: 10
    8:
      data_name: domain_med_snr_fast_linear_cl.mat
      snr: 10
  interpolation: thin_plate_spline
  normalisation: zscore
  normalise_target: True
  num_workers: 0
  validation_split: 0.15
  test_split: 0.15

training:
  epochs_per_task: 14  # Default for high SNR tasks
  batch_size: 32
  loss_function: mse
  optimiser: adam
  learning_rate: 0.001   # Higher default - scheduler will reduce this
  weight_decay: 0.0001
  
  # Task-specific learning rates (higher initial rates - CosineAnnealingLR will reduce them)
  task_learning_rates:
    0: 0.001   # High SNR (20 dB) - start higher
    1: 0.001   # High SNR (20 dB) - start higher  
    2: 0.001   # High SNR (20 dB) - start higher
    3: 0.0015  # Low SNR (3 dB) - even higher for difficult domains
    4: 0.0015  # Low SNR (3 dB) - even higher for difficult domains
    5: 0.0015  # Low SNR (3 dB) - even higher for difficult domains
    6: 0.0012  # Medium SNR (10 dB) - moderate increase
    7: 0.0012  # Medium SNR (10 dB) - moderate increase
    8: 0.0012  # Medium SNR (10 dB) - moderate increase
  
  # Task-specific epoch counts (longer training for harder domains)
  task_epochs:
    0: 30      # High SNR
    1: 30      # High SNR
    2: 30      # High SNR
    3: 50      # Low SNR (needs more epochs)
    4: 50      # Low SNR (needs more epochs)
    5: 50      # Low SNR (needs more epochs)
    6: 30      # Medium SNR
    7: 35      # Medium SNR
    8: 35      # Medium SNR
  
  task_weight_decays:
    0: 0.0001
    1: 0.0001
    2: 0.0001
    3: 0.0001
    4: 0.0001
    5: 0.0001
    6: 0.0001
    7: 0.0001
    8: 0.0001
  betas: [0.9, 0.999]
  early_stopping_patience: 3  # Increased from 5
  
  # CosineAnnealingLR scheduler with task-specific T_max
  scheduler:
    type: CosineAnnealingLR
    params:
      T_max: 4  # Default for high SNR
      eta_min: 0.00001  # Higher minimum to avoid too aggressive reduction
      verbose: true
  
  # Task-specific scheduler parameters
  task_scheduler_params:
    0: {T_max: 4, eta_min: 0.00001}   # High SNR - quick cosine cycle
    1: {T_max: 4, eta_min: 0.00001}   # High SNR - quick cosine cycle
    2: {T_max: 4, eta_min: 0.00001}   # High SNR - quick cosine cycle
    3: {T_max: 8, eta_min: 0.00001}   # Low SNR - longer cosine cycle
    4: {T_max: 8, eta_min: 0.00001}   # Low SNR - longer cosine cycle
    5: {T_max: 8, eta_min: 0.00001}   # Low SNR - longer cosine cycle
    6: {T_max: 6, eta_min: 0.00001}   # Medium SNR - medium cosine cycle
    7: {T_max: 6, eta_min: 0.00001}   # Medium SNR - medium cosine cycle
    8: {T_max: 6, eta_min: 0.00001}   # Medium SNR - medium cosine cycle

  # EWC (Elastic Weight Consolidation) parameters
  ewc:
    enabled: false
    lambda_ewc: 1000.0               # EWC regularization strength (set to 0.0 to disable EWC loss but still compute Fisher)
    fisher_samples: null             # Number of samples for Fisher computation (null = use all validation)
    compute_fisher_after_task: true  # Compute Fisher matrices after each task training

evaluation:
  metrics: [nmse, psnr, ssim, mse]
  plot_n_examples: 3

logging:
  checkpoint_dir: ./checkpoints/lora/

hardware:
  device: auto
  use_amp: True 